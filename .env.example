# ─── Obelisk Core Configuration ─────────────────────────────
# Copy this file to .env and fill in the values you need.
# All values have sensible defaults for local development.

# ─── Inference Service ──────────────────────────────────────
# The Python inference service hosts the LLM model.

# Bind address and port for the inference service
INFERENCE_HOST=127.0.0.1
INFERENCE_PORT=7780

# URL that agents/execution engine use to reach the inference service
# Local dev:  http://localhost:7780  (default)
# Production: https://inference.theobelisk.ai  (remote GPU server)
# INFERENCE_SERVICE_URL=http://localhost:7780

# Device for model loading (auto-detects: cuda > cpu)
# INFERENCE_DEVICE=cuda

# API key for inference service authentication
# When set, all /v1/* endpoints require this key via:
#   Authorization: Bearer <key>  or  X-API-Key: <key>
# Leave empty to disable auth (local dev).
# INFERENCE_API_KEY=

# Debug mode — enables verbose logging (full prompts, responses, token counts)
# INFERENCE_DEBUG=false

# ─── Telegram ───────────────────────────────────────────────
# Default Telegram bot credentials for development.
# In production, pass these as environment variables when deploying agents.

# TELEGRAM_DEV_AGENT_BOT_TOKEN=your_bot_token
# TELEGRAM_CHAT_ID=-your_chat_id
