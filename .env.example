# Obelisk Core Configuration

# Mode: "solo" (local) or "prod" (Supabase)
OBELISK_CORE_MODE=solo

# Storage path (solo mode only, defaults to ~/.obelisk-core/data)
# OBELISK_CORE_STORAGE_PATH=/path/to/data

# API Server Configuration
OBELISK_CORE_HOST=0.0.0.0
OBELISK_CORE_PORT=7779

# Debug mode (set to true to see full prompts and responses)
OBELISK_CORE_DEBUG=false

# Supabase Configuration (prod mode only)
# SUPABASE_URL=your_supabase_url
# SUPABASE_SERVICE_ROLE_KEY=your_service_role_key

# IBM Quantum Configuration (optional)
# IBM_QUANTUM_API_KEY=your_api_key
# IBM_QUANTUM_INSTANCE=your_instance

# Mistral AI Configuration (optional, for prod mode evolution)
# MISTRAL_API_KEY=your_api_key
# MISTRAL_AGENT_ID=your_agent_id
# MISTRAL_EVOLUTION_AGENT_ID=your_evolution_agent_id

# ─── Inference Service ─────────────────────────────────────
# Both obelisk-core and obelisk-inference read from this .env file.

# Inference service URL (obelisk-core uses this to call inference)
# Local dev:  http://localhost:7780  (default, inference running on same machine)
# Production: https://inference.theobelisk.ai  (remote GPU server)
# INFERENCE_SERVICE_URL=http://localhost:7780

# Inference service bind address (default: 127.0.0.1, set 0.0.0.0 for remote access)
# INFERENCE_HOST=127.0.0.1
# INFERENCE_PORT=7780

# Device for inference (auto-detects: cuda > cpu). Override explicitly if needed.
# INFERENCE_DEVICE=cuda

# API key for inference service auth (shared by both core and inference)
# When set, all /v1/* calls require Authorization: Bearer <key> or X-API-Key header.
# Leave empty to disable auth (local dev).
# INFERENCE_API_KEY=your-secret-key

# ─── Telegram ──────────────────────────────────────────────
# TELEGRAM_DEV_AGENT_BOT_TOKEN=your_bot_token
# TELEGRAM_CHAT_ID=-your_chat_id
