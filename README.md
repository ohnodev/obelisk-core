# Obelisk Core

<p align="center">
  <img src="assets/obelisk-logo.jpg" alt="Obelisk Core" width="400">
</p>

<p align="center">
  <strong>A simple Python framework for building AI agents with a self-hosted LLM and memory layer</strong>
</p>

<p align="center">
  <a href="https://github.com/ohnodev/Obelisk-core/releases"><img src="https://img.shields.io/badge/version-0.1.0--alpha-blue?style=for-the-badge" alt="Version"></a>
  <a href="LICENSE"><img src="https://img.shields.io/badge/License-MIT-blue.svg?style=for-the-badge" alt="MIT License"></a>
  <a href="https://github.com/ohnodev/Obelisk-core"><img src="https://img.shields.io/badge/Status-Alpha-yellow?style=for-the-badge" alt="Status"></a>
  <a href="https://www.python.org/downloads/"><img src="https://img.shields.io/badge/Python-3.8+-blue?style=for-the-badge&logo=python" alt="Python"></a>
</p>

**Obelisk Core** is a Python framework for building AI agents with a self-hosted LLM and memory layer. Start with the basics and extend with modules as needed.

**Status**: üü¢ Alpha - v0.1.0-alpha

> **Note**: This is an alpha release. The API may change in future versions.

This is the first basic version of the framework. It provides:
- **Self-hosted LLM** (Qwen3-0.6B) with thinking mode
- **Conversation memory** with LangChain
- **Dual storage modes** (local JSON / Supabase)
- **REST API** and CLI interface

[Quick Start](#quick-start) ¬∑ [Documentation](#documentation) ¬∑ [Contributing](CONTRIBUTING.md) ¬∑ [Security](SECURITY.md) ¬∑ [Changelog](CHANGELOG.md)

## ‚ú® Features

- **üß† Self-Hosted LLM**: Qwen3-0.6B model with thinking mode support (no external API calls)
- **üíæ Memory Layer**: LangChain-based conversation memory with automatic summarization
- **üîÑ Dual Mode**: Run in solo mode (local JSON) or prod mode (Supabase)
- **üåê HTTP API**: FastAPI REST API for integration
- **‚å®Ô∏è CLI Interface**: Command-line tools for development and testing
- **üß© Modular Design**: Tools and features can be added as modules
- **üîí Privacy-First**: All data stored locally in solo mode, no external API calls
- **üöÄ Easy Setup**: Simple installation, works out of the box

## üöÄ Quick Start

Get up and running in under 5 minutes:

```bash
# Clone and install
git clone https://github.com/ohnodev/Obelisk-core.git
cd obelisk-core
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install -e .

# Start chatting
obelisk-core chat
```

See [QUICKSTART.md](QUICKSTART.md) for detailed instructions.

## üì¶ Installation

```bash
# Clone the repository
git clone <repository-url>
cd obelisk-core

# Create virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies (this will take a few minutes - downloads ~2GB)
pip install -r requirements.txt

# Install the package so 'obelisk-core' command works
pip install -e .
```

**Note:** 
- If `pip` is not found, use `pip3` or `python3 -m pip` instead
- First installation downloads the Qwen3-0.6B model (~600MB) and dependencies (~2GB total)
- This may take 5-10 minutes depending on your internet connection

## Configuration

Copy `.env.example` to `.env` and configure:

```bash
cp .env.example .env
```

### Solo Mode (Default)

For local development and testing:

```env
OBELISK_CORE_MODE=solo
OBELISK_CORE_STORAGE_PATH=~/.obelisk-core/data/
```

### Prod Mode

For production with Supabase:

```env
OBELISK_CORE_MODE=prod
SUPABASE_URL=your_supabase_url
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key
```

### Optional Services

```env
# IBM Quantum (optional, for future quantum influence module)
IBM_QUANTUM_API_KEY=your_ibm_quantum_key
IBM_QUANTUM_INSTANCE=your_instance

# Mistral AI (optional, for future evolution evaluation)
MISTRAL_API_KEY=your_mistral_key
MISTRAL_AGENT_ID=your_agent_id
MISTRAL_EVOLUTION_AGENT_ID=your_evolution_agent_id
```

## Usage

### CLI

```bash
# Run API server
obelisk-core serve --port 7779 --mode solo

# Interactive chat (solo mode only)
obelisk-core chat

# Test LLM
obelisk-core test

# Show configuration
obelisk-core config

# Clear all local memory (fresh start, solo mode only)
obelisk-core clear

# Clear without confirmation prompt
obelisk-core clear --confirm
```

### API Server

```bash
# Start the server
obelisk-core serve

# Or use uvicorn directly
uvicorn src.api.server:app --host 0.0.0.0 --port 7779
```

### Python API

```python
from config import Config
from src.llm.obelisk_llm import ObeliskLLM
from src.storage import LocalJSONStorage

# Initialize storage
storage = Config.get_storage()

# Initialize LLM
llm = ObeliskLLM(storage=storage)

# Generate response (thinking mode enabled by default)
result = llm.generate(
    query="What is The Obelisk?",
    quantum_influence=0.7,
    enable_thinking=True  # Use thinking mode for complex reasoning
)

print(result['response'])
print(result.get('thinking_content'))  # View reasoning process

# Disable thinking mode for faster responses
result = llm.generate(
    query="Hello!",
    enable_thinking=False
)

# Dynamic thinking mode control via query
result = llm.generate(
    query="Solve this math problem /think"  # Forces thinking mode
)
result = llm.generate(
    query="Just say hi /no_think"  # Disables thinking mode
)
```

### Thinking Mode

Qwen3-0.6B supports **thinking mode** for enhanced reasoning on complex tasks:

- **Enabled by default** - The model uses thinking mode automatically for better quality
- **Dynamic control** - Use `/think` or `/no_think` in your query to toggle per-request
- **Programmatic control** - Set `enable_thinking=True/False` in the `generate()` method
- **Thinking content** - Access reasoning process via `result['thinking_content']`

Thinking mode is ideal for:
- Math problems and logical reasoning
- Complex questions requiring step-by-step analysis
- Code generation and debugging

Non-thinking mode is better for:
- Simple conversational responses
- Quick answers that don't need deep reasoning
- Faster response times

## Architecture

```
obelisk-core/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ llm/          # LLM inference (Qwen3-0.6B with thinking mode)
‚îÇ   ‚îú‚îÄ‚îÄ memory/       # Conversation memory management (LangChain)
‚îÇ   ‚îú‚îÄ‚îÄ storage/      # Storage abstraction (local JSON / Supabase)
‚îÇ   ‚îú‚îÄ‚îÄ api/          # FastAPI server and routes
‚îÇ   ‚îî‚îÄ‚îÄ cli/          # Command-line interface
‚îú‚îÄ‚îÄ config.py         # Configuration management
‚îú‚îÄ‚îÄ requirements.txt  # Python dependencies
‚îî‚îÄ‚îÄ setup.py          # Package setup
```

**Note**: Evolution, quantum, and other modules can be added as needed. The core framework provides LLM + memory as the foundation.

## Storage Modes

### Solo Mode

- Stores data in local JSON files (`~/.obelisk-core/data/`)
- Only accessible to the local user
- Perfect for development and testing
- No external dependencies

### Prod Mode

- Direct Supabase connection
- Shared data across users
- Production-ready
- Requires Supabase credentials

## üìö Documentation

- **[API Documentation](API.md)** - REST API endpoints and usage
- **[CLI Documentation](CLI.md)** - Command-line interface guide
- **[Quick Start Guide](QUICKSTART.md)** - Get started in 5 minutes
- **[Contributing](CONTRIBUTING.md)** - How to contribute
- **[Security](SECURITY.md)** - Security best practices
- **[Changelog](CHANGELOG.md)** - Version history

## üí¨ Example Usage

### Interactive Chat

```bash
obelisk-core chat
```

Example session:
```
‚óä THE OBELISK ‚óä
[ALPHA VERSION]

‚úì The Overseer is ready

Type 'quit' or 'exit' to end the conversation.

You: Hello, who are you?
‚óä The Overseer: [response]

You: My favorite color is green.
‚óä The Overseer: [acknowledgment]

You: What is my favorite color?
‚óä The Overseer: Your favorite color is green.
```

## üß™ Testing

Run the test suite to verify everything works:

```bash
# Run all tests
pytest tests/ -v

# Run specific test
pytest tests/test_basic.py::TestMemory::test_memory_storage -v

# With debug output
OBELISK_CORE_DEBUG=true pytest tests/ -v -s
```

The tests include:
- **Hello World**: Basic interaction test
- **Memory Test**: Tell the agent your favorite color, then ask it to recall it (verifies model output quality)
- **Multiple Interactions**: Test memory persistence across conversations

See [tests/README.md](tests/README.md) for more details.

## üõ†Ô∏è Development

```bash
# Install in development mode
pip install -e .

# Run tests
pytest tests/ -v

# Run linter (if configured)
flake8 src/
```

See [CONTRIBUTING.md](CONTRIBUTING.md) for development guidelines.

## Production Deployment

### Solo Mode (Local/Development)

Solo mode is perfect for development and single-user scenarios:

```bash
# Set mode to solo
OBELISK_CORE_MODE=solo

# Start API server
obelisk-core serve --port 7779
```

**Storage**: Data is stored locally in `~/.obelisk-core/data/` (configurable via `OBELISK_CORE_STORAGE_PATH`)

**Limitations**: 
- Single user only
- No multi-user support
- Data stored locally (not shared)

### Prod Mode (Supabase/Production)

For production deployments with multiple users:

```bash
# Set mode to prod
OBELISK_CORE_MODE=prod
SUPABASE_URL=your_supabase_url
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key

# Start API server
obelisk-core serve --port 7779
```

**Requirements**:
- Supabase project with required tables (see `supabase/schema.sql`)
- Service role key for database access
- Network access to Supabase

**Production Checklist**:
- [ ] Set `OBELISK_CORE_MODE=prod`
- [ ] Configure Supabase credentials
- [ ] Run database migrations
- [ ] Set up proper logging
- [ ] Configure reverse proxy (nginx, etc.) if needed
- [ ] Set up process manager (PM2, systemd, etc.)
- [ ] Configure monitoring/alerting
- [ ] Set up backups for Supabase database

### Security Considerations

- **Never commit `.env` files** - they're in `.gitignore`
- **Use environment variables** for all sensitive configuration
- **Rotate API keys regularly** in production
- **Use HTTPS** for API endpoints in production
- **Limit API access** with proper authentication/authorization
- **Monitor logs** for suspicious activity

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ü§ù Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on:

- Creating pull requests
- Code style and standards
- Testing requirements
- Areas where help is needed

## üìù Changelog

See [CHANGELOG.md](CHANGELOG.md) for a detailed list of changes and version history.

---

<p align="center">
  <strong>Built with ‚ù§Ô∏è for the open source community</strong>
</p>
